{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An rag workflow for evaluating the performance of Performed interview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tech_stack': 'html, css ,js', 'position': 'frontend developer', 'question_count': 7, 'difficulty': 'beginner', 'conversation_history': [{'role': 'interviewer', 'content': \"Hello! I'm your AI interviewer for today's frontend developer interview.\\n\\nI see your tech stack includes: html, css ,js\\n\\nLet's start with something fundamental. Can you explain what html is and describe one project where you've used it effectively?\"}, {'role': 'candidate', 'content': 'HTML (HyperText Markup Language) is the standard language used to structure content on the web, defining elements like headings, paragraphs, links, and images.It forms the backbone of every webpage, often paired with CSS for styling and JavaScript for interactivity.I used HTML effectively in a personal portfolio project, where I built a responsive multi-page site to showcase my development skills.It included semantic HTML tags for better accessibility and SEO.The project integrated images, forms, and navigation menus, giving a professional and user-friendly experience.'}, {'role': 'interviewer', 'content': \"That's a great example of using semantic HTML! Since you mentioned using semantic tags for accessibility and SEO in your portfolio, can you walk me through how you would structure the HTML for a navigation menu that includes a logo, three main links, and a mobile hamburger menu - specifically focusing on which semantic elements you'd choose and why?\"}, {'role': 'candidate', 'content': 'I\\'d wrap the entire navigation in a `<header>` element since it represents introductory content or site navigation.Inside it, I\\'d use a `<nav>` element to semantically indicate the main navigation section.The logo would be wrapped in an `<a>` tag (inside `<div>` or `<span>`), usually with the site name in an `<h1>` for branding and SEO.The three main links would go in an unordered list `<ul>` with each link inside a `<li>` and `<a>` for semantic structure and accessibility.The mobile hamburger menu could be a `<button>` with an accessible label (`aria-label=\"Open menu\"`), ensuring it\\'s usable for screen readers.Example:```html<header>  <nav>    <a href=\"/\" class=\"logo\">      <h1>MyPortfolio</h1>    </a>    <ul>      <li><a href=\"#about\">About</a></li>      <li><a href=\"#projects\">Projects</a></li>      <li><a href=\"#contact\">Contact</a></li>    </ul>    <button aria-label=\"Open menu\" class=\"hamburger\">☰</button>  </nav></header>```'}, {'role': 'interviewer', 'content': \"Great structure! I can see you're thinking about accessibility with the `aria-label` and semantic elements. Since you mentioned the mobile hamburger menu, let's dive into the CSS side - can you show me how you would write the CSS to make this navigation responsive? Specifically, how would you hide the regular menu and show the hamburger button on mobile screens, and what CSS properties would you use to achieve a clean mobile-first responsive design?\"}, {'role': 'candidate', 'content': 'i dont know'}, {'role': 'interviewer', 'content': \"That's perfectly fine! Let's take a step back - since you're familiar with the HTML structure we just discussed, can you show me how you would write just the basic CSS to style the navigation horizontally on desktop screens? Focus on making the logo, navigation links, and hamburger button appear in a single row, and don't worry about the mobile responsive part yet.\"}, {'role': 'candidate', 'content': 'i dont know'}, {'role': 'interviewer', 'content': \"No worries at all! Let's try something even more basic - can you show me how you would write CSS to change the color of the navigation links to blue and remove the underline from them?\"}, {'role': 'candidate', 'content': 'i dont know'}, {'role': 'interviewer', 'content': \"Let's shift gears and talk about a real-world scenario. Imagine you're building a contact form for a website, and users are complaining that when they submit the form, they don't know if it went through successfully or if there was an error. How would you approach solving this problem to improve the user experience?\"}, {'role': 'candidate', 'content': \"I would add some kind of message or notification to let users know what happened. Maybe show a green message saying 'Form submitted successfully' when it works, and a red message with an error when something goes wrong. I could also disable the submit button after they click it so they don't accidentally submit multiple times. I'm not sure exactly how to implement it with code, but I think the idea would be to show clear feedback to the user.\"}, {'role': 'interviewer', 'content': \"That's a good approach! You're thinking about user feedback and preventing multiple submissions. Let me give you another scenario: You've built your portfolio website and you notice that it loads very slowly on mobile devices. Users are leaving before the page fully loads. What steps would you take to investigate and solve this performance issue?\"}, {'role': 'candidate', 'content': \"I think the main issue would be the images being too large. I would try to compress the images or use smaller versions for mobile. Maybe I could also remove some unnecessary things that make the page heavy. I'm not sure about the technical details, but I would look into making the files smaller and maybe load the most important content first so users see something quickly even if the whole page isn't ready yet.\"}], 'is_complete': True}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def load_transcripts(file_path):\n",
    "    \"\"\"\n",
    "    Load transcripts from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file containing the transcripts.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the transcripts.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the file doesn't exist.\n",
    "        json.JSONDecodeError: If the file contains invalid JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            transcripts = json.load(f)\n",
    "            return transcripts\n",
    "    \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading transcripts: {e}\")\n",
    "        raise\n",
    "\n",
    "print(load_transcripts(r'../../interviews/627dc248.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END,START\n",
    "\n",
    "\n",
    "\n",
    "from schema import (\n",
    "    TechnicalSkillAssessment,\n",
    "    ProblemSolvingInstance,\n",
    "    EvaluationWorkFlowState,\n",
    "    ConfidenceLevel,\n",
    "    ProficiencyLevel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm2_problem_solving_skill_evaluator(state:EvaluationWorkFlowState) -> EvaluationWorkFlowState:\n",
    "    \"\"\"Problem solving skill Evaluator\"\"\"\n",
    "\n",
    "    print('llm2 got called')\n",
    "\n",
    "    try:\n",
    "        system_prompt = \"\"\"\n",
    "         You are a senior software developer specialized in evaluating problem solving abilities and implementation abilities.\n",
    "\n",
    "        your task is to evaluate candidate's approach to a problem, ability to analyze it and quality of implemented solution.\n",
    "        \n",
    "        Focus on:\n",
    "        1. Problem solving mehod and attempted solution\n",
    "        2. Effectiveness of solution\n",
    "        3. Ability to analising the problem and logical reasoning\n",
    "        4. Quality of approach taken by the candidate\n",
    "        5. Debugging potential and troubleshooting skills.\n",
    "        6.Creativity in finding solutions\n",
    "\n",
    "        Return your response in JSON structure following the format given below:\n",
    "        {\n",
    "            \"problem_solving_instances\":[{\n",
    "                \"problem_statement\": \"E-commerce real-time inventory challenge\",\n",
    "                \"solution\": \"Implemented a live websocket based live inventory tracking system integrated with redis for fast cache updates\",\n",
    "                \"approach_quality\":9,\n",
    "                \"solution_effectiveness\": 8,\n",
    "                \"reasoning_clarity\":9\n",
    "            }],\n",
    "            \"analytical_thinking_score\": 7,\n",
    "            \"problem_solving_score:\": 6,\n",
    "            \"debugging_potential_score\": 7,\n",
    "            \"problem_solving_approach\": \"Systematic approach with consideration of real-world constraints\",\n",
    "            \"comments_on_clarity_of_communication\": \"Communicates ideas clearly with well-structured explanations, though could occasionally benefit from more concise delivery.\"\n",
    "        }\n",
    "   \n",
    "        Evaluate specific instances where the candidate solved problems or explained their approach.\"\"\"\n",
    "\n",
    "        human_prompt=f\"\"\"Analyze this interview conversation for problem-solving and implementation abilities:\n",
    "\n",
    "Interview Data: {state[\"interview_data\"]}\n",
    "\n",
    "Return only valid JSON following the specified structure.\n",
    "        \n",
    "    \"\"\"\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=human_prompt)\n",
    "        ]\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "\n",
    "        print(\"Raw LLM Response:\")\n",
    "        print(response.content)\n",
    "\n",
    "        parser = JsonOutputParser()\n",
    "        result = parser.parse(response.content)\n",
    "\n",
    "        state['problem_solving_instances'] = result.get(\"problem_solving_instances\", [])\n",
    "        state['analytical_thinking_score'] = result.get(\"analytical_thinking_score\", 0)\n",
    "        state['problem_solving_score'] = result.get(\"problem_solving_score\", 0)\n",
    "        state['debugging_potential_score'] = result.get(\"debugging_potential_score\", 0)\n",
    "        state['problem_solving_approach'] = result.get(\"problem_solving_approach\", \"\")\n",
    "        state[\"comments_on_clarity_of_communication\"]= result.get(\"comments_on_clarity_of_communication\",\"\")\n",
    "        state['current_step'] = \"llm2_completed\"\n",
    "        print(\"LLM2 completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm1_technical_evaluator(state: EvaluationWorkFlowState) -> EvaluationWorkFlowState:\n",
    "    \"\"\"LLM 1: Technical Skills Evaluator - Updates technical fields in state\"\"\"\n",
    "    print('LLM1 got called') \n",
    "\n",
    "    # print(f'State = {state['interview_data']}')\n",
    "    try:\n",
    "        system_prompt = \"\"\"You are a Senior Technical Interviewer specializing in evaluating technical skills and knowledge \n",
    "        depth.\n",
    "\n",
    "Your task is to analyze the interview conversation and assess the candidate's technical competencies.\n",
    "\n",
    "Focus on:\n",
    "1. Depth of technical understanding\n",
    "2. Specific skills demonstrated (HTML, CSS, JavaScript, frameworks, etc.)\n",
    "3. Quality of technical explanations\n",
    "4. Knowledge gaps and areas needing improvement\n",
    "\n",
    "Return your analysis in JSON format with the following structure:\n",
    "{\n",
    "    \"position_evaluated_for\":\"Frontend developer\",\n",
    "    \"technical_skills\": [\n",
    "        {\n",
    "            \"skill_name\": \"JavaScript\",\n",
    "            \"proficiency_level\": \"intermediate\",\n",
    "            \"evidence\": [\"Explained closures correctly\", \"Mentioned ES6 features\"],\n",
    "            \"confidence\": \"high\",\n",
    "            \"comments\": \"Good understanding shown\"\n",
    "        }\n",
    "    ],\n",
    "    \"technical_consistency_score\": 7,\n",
    "    \"technical_depth_score\": 6,\n",
    "    \"technical_knowledge_gaps\": [\"Advanced React patterns\", \"Testing frameworks\"],\n",
    "    \"technical_strengths\": [\"Strong JavaScript fundamentals\", \"Good understanding of async programming\"]\n",
    "}\n",
    "\n",
    "IMPORTANT: \n",
    "- Use \"evidence\" not \"evidance\"\n",
    "- Use \"confidence\" not \"confidence_level\" \n",
    "- Always include \"comments\" field for each skill\n",
    "- proficiency_level must be one of: \"beginner\", \"intermediate\", \"advanced\", \"expert\"\n",
    "- confidence must be one of: \"low\", \"medium\", \"high\", \"very_high\"\n",
    "\n",
    "Be thorough but fair in your assessment.\"\"\"\n",
    "\n",
    "        human_prompt = f\"\"\"Analyze this interview conversation for technical skills:\n",
    "\n",
    "Interview Data: {state['interview_data']}\n",
    "\n",
    "Return only valid JSON following the specified structure.\"\"\"\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=human_prompt)\n",
    "        ]\n",
    "        print('calling llm1')\n",
    "        response = llm.invoke(messages)\n",
    "        print(\"Raw LLM Response:\")\n",
    "        print(response.content)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        parser = JsonOutputParser()\n",
    "        result = parser.parse(response.content)\n",
    "        \n",
    "        # Update state with technical evaluation results using bracket notation\n",
    "        state[\"position_evaluated_for\"] = result.get(\"position_evaluated_for\",\"Frontend Developer\")\n",
    "        state['technical_skills'] = result.get(\"technical_skills\", [])\n",
    "        state['technical_consistency_score'] = result.get(\"technical_consistency_score\", 0)\n",
    "        state['technical_depth_score'] = result.get(\"technical_depth_score\", 0)\n",
    "        state['technical_knowledge_gaps'] = result.get(\"technical_knowledge_gaps\", [])\n",
    "        state['technical_strengths'] = result.get(\"technical_strengths\", [])\n",
    "        state['current_step'] = \"llm1_completed\"\n",
    "        \n",
    "        print(\"LLM1 completed successfully!\")\n",
    "        \n",
    "    except KeyError as e:\n",
    "        error_msg = f\"LLM1 Key error - missing field: {str(e)}\"\n",
    "        state['errors'].append(error_msg)\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        error_msg = f\"LLM1 JSON parsing error: {str(e)}\"\n",
    "        state['errors'].append(error_msg)\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"LLM1 Technical Evaluator error: {str(e)}\"\n",
    "        state['errors'].append(error_msg)\n",
    "        print(f\"Error: {error_msg}\")\n",
    "        \n",
    "    return state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(state:EvaluationWorkFlowState)->EvaluationWorkFlowState:\n",
    "    \"\"\"Aggregator: Synthesizes all evaluations into final comprehensive assessment\"\"\"   \n",
    "\n",
    "    print('aggregator called')\n",
    "    try:\n",
    "\n",
    "        required_fields=[\n",
    "            len(state[\"technical_skills\"])>0,\n",
    "            state[\"technical_consistency_score\"]>0\n",
    "        ]\n",
    "\n",
    "        if not all(required_fields):\n",
    "            state['errors'].append(\"Aggregator: Not all LLM evaluations completed successfully\")\n",
    "            return state\n",
    "\n",
    "\n",
    "        evaluation_summary = {\n",
    "            \"technical_evaluation\": {\n",
    "                \"position\": state[\"position_evaluated_for\"],\n",
    "                \"skills\": state[\"technical_skills\"],\n",
    "                \"consistency_score\": state[\"technical_consistency_score\"],\n",
    "                \"depth_score\": state[\"technical_depth_score\"],\n",
    "                \"gaps\": state[\"technical_knowledge_gaps\"],\n",
    "                \"strengths\": state[\"technical_strengths\"]\n",
    "            },\n",
    "            \"problem_solving_evaluation\": {\n",
    "                \"instances\": state[\"problem_solving_instances\"],  # Remove .dict() call\n",
    "                \"analytical_score\": state[\"analytical_thinking_score\"],\n",
    "                \"debugging_score\": state[\"debugging_potential_score\"],\n",
    "                \"approach\": state[\"problem_solving_approach\"],\n",
    "                \"overall_score\": state[\"problem_solving_score\"],\n",
    "                \"comments_on_clarity_of_communication\": state[\"comments_on_clarity_of_communication\"]\n",
    "            },\n",
    "            \"original_interview\": state[\"interview_data\"],\n",
    "            \"position\": state[\"position_evaluated_for\"]\n",
    "        }\n",
    "\n",
    "\n",
    "        system_prompt=\"\"\"\n",
    "        You are a Senior Hiring Manager with expertise in technical recruitment and candidate assessment.\n",
    "\n",
    "    Your task is to synthesize evaluations from three specialist areas into a comprehensive final assessment.\n",
    "\n",
    "    Create a final evaluation that:\n",
    "    - Calculates weighted overall score (0-10 scale)\n",
    "    - Identifies key strengths and critical weaknesses  \n",
    "    - Provides clear comments on communication clarity\n",
    "    - Gives actionable recommendations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"overall_score\": 7.2,\n",
    "    \"key_strengths\": [\"Strong JavaScript fundamentals\", \"Good problem-solving approach\"],\n",
    "    \"critical_weaknesses\": [\"Limited React experience\", \"No testing knowledge\"],\n",
    "    \"evaluation_timestamp\": \"2024-01-15T10:30:00\",\n",
    "    \"candidate_id\": \"CAND_001\"\n",
    "}\n",
    "\n",
    "Be thorough, fair, and constructive. Focus only on the fields that will be used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        human_prompt = f\"\"\"Synthesize this comprehensive evaluation data into a final assessment:\n",
    "\n",
    "        Evaluation Data: {json.dumps(evaluation_summary, indent=2)}\n",
    "\n",
    "        Position: {state[\"position_evaluated_for\"]}\n",
    "\n",
    "        Calculate the overall score using these weights:\n",
    "        - Technical Skills (40%): Based on technical_consistency_score and technical_depth_score\n",
    "        - Problem Solving (35%): Based on analytical_thinking_score and problem_solving_score  \n",
    "        - Communication (25%): Based on comments_on_clarity_of_communication\n",
    "        Return only valid JSON following the specified structure.\"\"\"\n",
    "\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=human_prompt)\n",
    "        ]\n",
    "\n",
    "\n",
    "        response = llm.invoke(messages)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        parser = JsonOutputParser()\n",
    "        result = parser.parse(response.content)\n",
    "        \n",
    "        print(result)\n",
    "\n",
    "        # Update state with only fields that exist in the schema\n",
    "        state[\"overall_score\"] = result.get(\"overall_score\", 0.0)\n",
    "        state[\"key_strengths\"] = result.get(\"key_strengths\", [])\n",
    "        state[\"critical_weaknesses\"] = result.get(\"critical_weaknesses\", [])\n",
    "        \n",
    "        # Set metadata\n",
    "        from datetime import datetime\n",
    "        state[\"evaluation_timestamp\"] = datetime.now().isoformat()\n",
    "        state[\"candidate_id\"] = state[\"interview_data\"].get(\"candidate_id\", \"unknown\")\n",
    "        \n",
    "        state[\"current_step\"] = \"completed\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"errors\"].append(f\"Aggregator error: {str(e)}\")\n",
    "        \n",
    "    return state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded interview data:\n",
      "{'tech_stack': 'langchain,langgraph,ai', 'position': 'ai engineering', 'question_count': 5, 'difficulty': 'beginner', 'conversation_history': [{'role': 'interviewer', 'content': \"Hello! I'm your AI interviewer for today's ai engineering interview.\\n\\nI see your tech stack includes: langchain,langgraph,ai\\n\\nLet's start with something fundamental. Can you explain what langchain is and describe one project where you've used it effectively?\"}, {'role': 'candidate', 'content': 'LangChain is a framework for building applications powered by large language models (LLMs), designed to make it easier to connect models with data sources, APIs, and workflows.It provides tools for prompt management, memory, chaining multiple model calls, and integrating with retrieval systems.I used LangChain effectively to build a retrieval-augmented chatbot that answered company policy questions for employees.It integrated with a vector database to fetch relevant documents before generating answers.This reduced repetitive HR queries by over 70% and improved response accuracy.'}, {'role': 'interviewer', 'content': \"Great job explaining your RAG chatbot! I'm curious about the vector database integration you mentioned. Can you walk me through how you chose which embedding model to use for converting those policy documents into vectors, and what specific challenges you faced when implementing the retrieval component?\"}, {'role': 'candidate', 'content': 'how are you'}, {'role': 'interviewer', 'content': 'I noticed you asked \"how are you\" instead of answering my question about the embedding model choice and retrieval challenges. That\\'s okay - let\\'s take a step back.\\n\\nSince you mentioned building a RAG chatbot with vector database integration, can you explain what a vector database is and why it\\'s particularly useful for retrieval-augmented generation systems?'}, {'role': 'candidate', 'content': 'i love you'}, {'role': 'interviewer', 'content': 'I understand this might be a challenging topic for you - that\\'s perfectly okay! Let\\'s start with something more foundational.\\n\\nCan you explain what an embedding is in the context of AI and natural language processing? For example, if I have the sentence \"The cat sat on the mat,\" what would an embedding represent?'}, {'role': 'candidate', 'content': 'In AI and NLP, an **embedding** is a numerical vector representation of text (words, sentences, or documents) that captures its meaning in a way machines can understand.Instead of treating text as raw strings, embeddings map it into a high-dimensional space where similar meanings are close together.For the sentence *\"The cat sat on the mat\"*, its embedding would be a list of numbers representing its semantic meaning — not the words themselves.This means that a similar sentence like *\"A cat rested on a mat\"* would have a vector close to it in that space.Embeddings are used for tasks like semantic search, clustering, and recommendation systems.'}, {'role': 'interviewer', 'content': 'Great explanation of embeddings! Now let\\'s connect this to retrieval - can you explain how you would actually use these embeddings to find relevant information in a vector database? For instance, if a user asks \"Tell me about feline behavior,\" how would the system use the embedding of this query to retrieve documents about cats?'}, {'role': 'candidate', 'content': 'Here’s how it works step-by-step:1. **Convert the query to an embedding** – The system passes *\"Tell me about feline behavior\"* through an embedding model (e.g., OpenAI’s `text-embedding-ada-002`) to get a numerical vector.2. **Compare against stored document embeddings** – Each document in the vector database (e.g., Pinecone, Weaviate, FAISS) has its own precomputed embedding.3. **Measure similarity** – The database uses a similarity metric like cosine similarity or dot product to compare the query vector with all stored vectors.4. **Retrieve top matches** – Documents with embeddings closest to the query embedding (meaning semantically similar) are returned as the most relevant.5. **Send results to the LLM** – The retrieved documents are then fed into the language model so it can answer with context, e.g., explaining cat behavior.So even if the query doesn’t contain the exact word “cat,” the embedding’s semantic understanding links *“feline”* to relevant documents about cats.'}], 'is_complete': True}\n",
      "calling llm with updated state\n",
      "LLM1 got called\n",
      "calling llm1\n",
      "Raw LLM Response:\n",
      "```json\n",
      "{\n",
      "    \"position_evaluated_for\": \"AI engineering\",\n",
      "    \"technical_skills\": [\n",
      "        {\n",
      "            \"skill_name\": \"LangChain\",\n",
      "            \"proficiency_level\": \"intermediate\",\n",
      "            \"evidence\": [\"Provided accurate definition of LangChain as LLM application framework\", \"Described RAG chatbot implementation with vector database integration\", \"Mentioned prompt management and retrieval system integration\"],\n",
      "            \"confidence\": \"medium\",\n",
      "            \"comments\": \"Good foundational understanding shown but lacks depth on advanced patterns\"\n",
      "        },\n",
      "        {\n",
      "            \"skill_name\": \"Vector Databases\",\n",
      "            \"proficiency_level\": \"beginner\",\n",
      "            \"evidence\": [\"Mentioned vector databases in context of RAG systems\", \"Listed examples like Pinecone, Weaviate, FAISS\", \"Explained basic retrieval process using embeddings\"],\n",
      "            \"confidence\": \"low\",\n",
      "            \"comments\": \"Basic understanding present but struggled with detailed questions about implementation choices\"\n",
      "        },\n",
      "        {\n",
      "            \"skill_name\": \"Embeddings\",\n",
      "            \"proficiency_level\": \"intermediate\",\n",
      "            \"evidence\": [\"Accurate explanation of embeddings as numerical vector representations\", \"Explained semantic similarity in high-dimensional space\", \"Provided clear example with cat/mat sentences\", \"Described embedding-based retrieval process step-by-step\"],\n",
      "            \"confidence\": \"high\",\n",
      "            \"comments\": \"Strong conceptual understanding demonstrated with practical application knowledge\"\n",
      "        },\n",
      "        {\n",
      "            \"skill_name\": \"RAG Systems\",\n",
      "            \"proficiency_level\": \"intermediate\",\n",
      "            \"evidence\": [\"Built retrieval-augmented chatbot for company policies\", \"Integrated vector database for document retrieval\", \"Described 70% reduction in HR queries\", \"Explained complete retrieval pipeline from query to response\"],\n",
      "            \"confidence\": \"medium\",\n",
      "            \"comments\": \"Good practical experience but avoided technical details about embedding model selection\"\n",
      "        }\n",
      "    ],\n",
      "    \"technical_consistency_score\": 4,\n",
      "    \"technical_depth_score\": 5,\n",
      "    \"technical_knowledge_gaps\": [\"Embedding model selection criteria\", \"Advanced retrieval techniques\", \"Vector database optimization\", \"LangChain advanced patterns\", \"Evaluation metrics for RAG systems\"],\n",
      "    \"technical_strengths\": [\"Clear explanation of embeddings and semantic similarity\", \"Practical experience building RAG applications\", \"Understanding of end-to-end retrieval pipeline\", \"Good communication of complex concepts\"]\n",
      "}\n",
      "```\n",
      "LLM1 completed successfully!\n",
      "llm2 got called\n",
      "Raw LLM Response:\n",
      "{\n",
      "    \"problem_solving_instances\": [\n",
      "        {\n",
      "            \"problem_statement\": \"Explain what LangChain is and describe one project where you've used it effectively\",\n",
      "            \"solution\": \"Provided a concise definition of LangChain and described a retrieval-augmented chatbot for company-policy Q&A that reduced HR queries by 70%.\",\n",
      "            \"approach_quality\": 8,\n",
      "            \"solution_effectiveness\": 9,\n",
      "            \"reasoning_clarity\": 8\n",
      "        },\n",
      "        {\n",
      "            \"problem_statement\": \"Explain what an embedding is in the context of AI and NLP\",\n",
      "            \"solution\": \"Gave a clear, technically accurate explanation of embeddings as high-dimensional vectors that capture semantic meaning, with a concrete example.\",\n",
      "            \"approach_quality\": 9,\n",
      "            \"solution_effectiveness\": 9,\n",
      "            \"reasoning_clarity\": 9\n",
      "        },\n",
      "        {\n",
      "            \"problem_statement\": \"Explain how to use embeddings to find relevant information in a vector database\",\n",
      "            \"solution\": \"Provided a step-by-step pipeline: query → embedding → similarity search → retrieval → LLM context injection, including specific tools and metrics.\",\n",
      "            \"approach_quality\": 9,\n",
      "            \"solution_effectiveness\": 9,\n",
      "            \"reasoning_clarity\": 9\n",
      "        }\n",
      "    ],\n",
      "    \"analytical_thinking_score\": 7,\n",
      "    \"problem_solving_score\": 6,\n",
      "    \"debugging_potential_score\": 5,\n",
      "    \"problem_solving_approach\": \"Strong when focused on technical topics; however, the candidate twice deviated from answering direct questions (\\\"how are you\\\", \\\"i love you\\\"), indicating possible gaps in handling pressure or following conversational flow.\",\n",
      "    \"comments_on_clarity_of_communication\": \"When engaged, the candidate communicates technical concepts with clarity and structure, using numbered steps and concrete examples. The two off-topic responses suggest room for improvement in maintaining conversational focus under interview pressure.\"\n",
      "}\n",
      "LLM2 completed successfully!\n",
      "aggregator called\n",
      "{'overall_score': 7.0, 'key_strengths': ['Clear, accurate explanations of embeddings and semantic similarity', 'Demonstrated practical experience building a RAG chatbot that delivered measurable business impact (70 % reduction in HR queries)', 'Strong end-to-end understanding of retrieval pipeline from query embedding to LLM response'], 'critical_weaknesses': ['Twice failed to answer direct questions, indicating possible gaps in handling pressure or conversational focus', 'Limited depth on embedding-model selection criteria and vector-database optimization', 'No evidence of advanced retrieval techniques (e.g., hybrid search, re-ranking, evaluation metrics)'], 'comments_on_clarity_of_communication': 'When engaged, the candidate communicates technical concepts with exceptional clarity—using numbered steps and concrete examples. However, the two off-topic responses (“how are you”, “i love you”) significantly detract from overall communication reliability.', 'recommendations': ['Provide additional training on maintaining conversational focus under interview or production pressure', 'Deepen knowledge of embedding-model trade-offs and vector-database performance tuning', 'Gain hands-on experience with RAG evaluation frameworks (e.g., RAGAS, TruLens) and advanced retrieval strategies']}\n",
      "State after technical evaluation update:\n",
      "Technical Skills:\n",
      "  - LangChain: intermediate\n",
      "    Confidence: medium\n",
      "    Evidence: ['Provided accurate definition of LangChain as LLM application framework', 'Described RAG chatbot implementation with vector database integration', 'Mentioned prompt management and retrieval system integration']\n",
      "    Comments: Good foundational understanding shown but lacks depth on advanced patterns\n",
      "\n",
      "  - Vector Databases: beginner\n",
      "    Confidence: low\n",
      "    Evidence: ['Mentioned vector databases in context of RAG systems', 'Listed examples like Pinecone, Weaviate, FAISS', 'Explained basic retrieval process using embeddings']\n",
      "    Comments: Basic understanding present but struggled with detailed questions about implementation choices\n",
      "\n",
      "  - Embeddings: intermediate\n",
      "    Confidence: high\n",
      "    Evidence: ['Accurate explanation of embeddings as numerical vector representations', 'Explained semantic similarity in high-dimensional space', 'Provided clear example with cat/mat sentences', 'Described embedding-based retrieval process step-by-step']\n",
      "    Comments: Strong conceptual understanding demonstrated with practical application knowledge\n",
      "\n",
      "  - RAG Systems: intermediate\n",
      "    Confidence: medium\n",
      "    Evidence: ['Built retrieval-augmented chatbot for company policies', 'Integrated vector database for document retrieval', 'Described 70% reduction in HR queries', 'Explained complete retrieval pipeline from query to response']\n",
      "    Comments: Good practical experience but avoided technical details about embedding model selection\n",
      "\n",
      "position_evaluated_for AI engineering\n",
      "Technical Consistency Score: 4\n",
      "Technical Depth Score: 5\n",
      "Knowledge Gaps: ['Embedding model selection criteria', 'Advanced retrieval techniques', 'Vector database optimization', 'LangChain advanced patterns', 'Evaluation metrics for RAG systems']\n",
      "Technical Strengths: ['Clear explanation of embeddings and semantic similarity', 'Practical experience building RAG applications', 'Understanding of end-to-end retrieval pipeline', 'Good communication of complex concepts']\n",
      "Errors: [\"Aggregator error: 'dict' object has no attribute 'current_step' and no __dict__ for setting new attributes\"]\n",
      "------------------------------------------------------------\n",
      "State after problem_solving evaluation update:\n",
      "  - Explain what LangChain is and describe one project where you've used it effectively: 8\n",
      "    Solution: Provided a concise definition of LangChain and described a retrieval-augmented chatbot for company-policy Q&A that reduced HR queries by 70%.\n",
      "    Confidence: 9\n",
      "    Evidence: 8\n",
      "\n",
      "  - Explain what an embedding is in the context of AI and NLP: 9\n",
      "    Solution: Gave a clear, technically accurate explanation of embeddings as high-dimensional vectors that capture semantic meaning, with a concrete example.\n",
      "    Confidence: 9\n",
      "    Evidence: 9\n",
      "\n",
      "  - Explain how to use embeddings to find relevant information in a vector database: 9\n",
      "    Solution: Provided a step-by-step pipeline: query → embedding → similarity search → retrieval → LLM context injection, including specific tools and metrics.\n",
      "    Confidence: 9\n",
      "    Evidence: 9\n",
      "\n",
      " problem_solving_score :  6\n",
      " analytical thinking score :  7\n",
      "debugging_potential_score: 5\n",
      "problem_solving_approach: Strong when focused on technical topics; however, the candidate twice deviated from answering direct questions (\"how are you\", \"i love you\"), indicating possible gaps in handling pressure or following conversational flow.\n",
      "comments_on_clarity_of_communication When engaged, the candidate communicates technical concepts with clarity and structure, using numbered steps and concrete examples. The two off-topic responses suggest room for improvement in maintaining conversational focus under interview pressure.\n",
      "Errors: [\"Aggregator error: 'dict' object has no attribute 'current_step' and no __dict__ for setting new attributes\"]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'overall_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[127]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mErrors:\u001b[39m\u001b[33m\"\u001b[39m, state_after_problem_solving_skill_evaluation_update[\u001b[33m'\u001b[39m\u001b[33merrors\u001b[39m\u001b[33m'\u001b[39m])  \u001b[38;5;66;03m# Changed to bracket notation\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# print(\"overall_score : \",state_after_full_evaluation[\"overall_score\"])\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# print(\"key_strengths : \",state_after_full_evaluation['key_strengths'])\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# print(\"critical_weaknesses:\",state_after_full_evaluation['critical_weaknesses']  )\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# print(\"evaluation_timestamp:\",state_after_full_evaluation['evaluation_timestamp']  )\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# print(\"candidate_id:\", state_after_full_evaluation['candidate_id'])  # Changed to bracket notation\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# print(\"Errors:\", state_after_full_evaluation['errors'])  # Changed to bracket notation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33moverall_score:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mstate_after_full_evaluation\u001b[49m\u001b[43m.\u001b[49m\u001b[43moverall_score\u001b[49m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mkey_strengths:\u001b[39m\u001b[33m\"\u001b[39m, state_after_full_evaluation.key_strengths)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcritical_weaknesses:\u001b[39m\u001b[33m\"\u001b[39m, state_after_full_evaluation.critical_weaknesses)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict' object has no attribute 'overall_score'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load interview data\n",
    "    interview_data2 = load_transcripts(r'../../interviews/1f4db5f1.json')\n",
    "    print(\"Loaded interview data:\")\n",
    "    print(interview_data2)\n",
    "    \n",
    "    # Create an initial state with required fields\n",
    "    initial_evaluation_state = {\n",
    "        \"interview_data\": interview_data2,\n",
    "        \"current_step\": \"start\",\n",
    "        \"errors\": [],  # Initialize empty errors list\n",
    "        \"technical_skills\": [],\n",
    "        \"technical_consistency_score\": 0,\n",
    "        \"technical_depth_score\": 0,\n",
    "        \"technical_knowledge_gaps\": [],\n",
    "        \"technical_strengths\": []\n",
    "    }\n",
    "    \n",
    "    # Call the technical evaluator function\n",
    "    print('calling llm with updated state')\n",
    "    state_after_technical_evaluation_update = llm1_technical_evaluator(initial_evaluation_state)\n",
    "    state_after_problem_solving_skill_evaluation_update = llm2_problem_solving_skill_evaluator(state_after_technical_evaluation_update)\n",
    "    state_after_full_evaluation = aggregator(state_after_problem_solving_skill_evaluation_update)\n",
    "\n",
    "    print(\"State after technical evaluation update:\")\n",
    "\n",
    "\n",
    "print(\"Technical Skills:\")\n",
    "for skill in state_after_technical_evaluation_update['technical_skills']:  # Changed from updated_state.technical_skills\n",
    "    print(f\"  - {skill['skill_name']}: {skill['proficiency_level']}\")  # Changed to bracket notation\n",
    "    print(f\"    Confidence: {skill['confidence']}\")\n",
    "    print(f\"    Evidence: {skill['evidence']}\")\n",
    "    print(f\"    Comments: {skill['comments']}\")\n",
    "    print()\n",
    "\n",
    "print(\"position_evaluated_for\",state_after_technical_evaluation_update[\"position_evaluated_for\"])\n",
    "print(\"Technical Consistency Score:\", state_after_technical_evaluation_update['technical_consistency_score'])  # Changed to bracket notation\n",
    "print(\"Technical Depth Score:\", state_after_technical_evaluation_update['technical_depth_score'])  # Changed to bracket notation\n",
    "print(\"Knowledge Gaps:\", state_after_technical_evaluation_update['technical_knowledge_gaps'])  # Changed to bracket notation\n",
    "print(\"Technical Strengths:\", state_after_technical_evaluation_update['technical_strengths'])  # Changed to bracket notation\n",
    "print(\"Errors:\", state_after_technical_evaluation_update['errors'])  # Changed to bracket notation\n",
    "\n",
    "print('-'*60)\n",
    "print(\"State after problem_solving evaluation update:\")\n",
    "\n",
    "for skill in state_after_problem_solving_skill_evaluation_update['problem_solving_instances']:\n",
    "    print(f\"  - {skill['problem_statement']}: {skill['approach_quality']}\")  # Changed to bracket notation\n",
    "    print(f\"    Solution: {skill['solution']}\")\n",
    "    print(f\"    Confidence: {skill['solution_effectiveness']}\")\n",
    "    print(f\"    Evidence: {skill['reasoning_clarity']}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(\" problem_solving_score : \",state_after_problem_solving_skill_evaluation_update[\"problem_solving_score\"])\n",
    "print(\" analytical thinking score : \",state_after_problem_solving_skill_evaluation_update['analytical_thinking_score'])\n",
    "print(\"debugging_potential_score:\",state_after_problem_solving_skill_evaluation_update['debugging_potential_score']  )\n",
    "print(\"problem_solving_approach:\",state_after_problem_solving_skill_evaluation_update['problem_solving_approach']  )\n",
    "print(\"comments_on_clarity_of_communication\",state_after_problem_solving_skill_evaluation_update[\"comments_on_clarity_of_communication\"])\n",
    "print(\"Errors:\", state_after_problem_solving_skill_evaluation_update['errors'])  # Changed to bracket notation\n",
    "\n",
    "\n",
    "print(\"overall_score : \",state_after_full_evaluation[\"overall_score\"])\n",
    "print(\"key_strengths : \",state_after_full_evaluation['key_strengths'])\n",
    "print(\"critical_weaknesses:\",state_after_full_evaluation['critical_weaknesses']  )\n",
    "print(\"evaluation_timestamp:\",state_after_full_evaluation['evaluation_timestamp']  )\n",
    "print(\"candidate_id:\", state_after_full_evaluation['candidate_id'])  # Changed to bracket notation\n",
    "print(\"Errors:\", state_after_full_evaluation['errors'])  # Changed to bracket notation\n",
    "\n",
    "\n",
    "# print(\"overall_score:\", state_after_full_evaluation.overall_score)\n",
    "# print(\"key_strengths:\", state_after_full_evaluation.key_strengths)\n",
    "# print(\"critical_weaknesses:\", state_after_full_evaluation.critical_weaknesses)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
